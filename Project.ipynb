{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#CS109 Final Project\n",
      "###Ben Zauzmer, Daniel Taylor, Jonathan Marks, and Michael Suo\n",
      "\n",
      "---\n",
      "\n",
      "###Background\n",
      "Various interesting things about news and elections go here...\n",
      "\n",
      "###Methods\n",
      "Various interesting things about our methods go here...\n",
      "\n",
      "###Results\n",
      "Various results...\n",
      "\n",
      "###Conculusions\n",
      "Things we discovered are the most important things ever discovered..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data Collection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from collections import defaultdict\n",
      "import json\n",
      "import requests\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import pprint\n",
      "\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib as mpl\n",
      "\n",
      "import random\n",
      "import math\n",
      "from scipy import stats\n",
      "import brewer2mpl\n",
      "\n",
      "#colorbrewer2 Dark2 qualitative color table\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'white'\n",
      "rcParams['patch.facecolor'] = dark2_colors[0]\n",
      "rcParams['font.family'] = 'StixGeneral'\n",
      "\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()\n",
      "        \n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# DATA\n",
      "\n",
      "# New York Times API\n",
      "def parse_NYT_query(data):\n",
      "    '''\n",
      "    Takes an inputted JSON-loaded string of NYT API results and parses it into a DataFrame\n",
      "    \n",
      "    Input: JSON-loaded NYT response string\n",
      "    Output: Pandas DataFrame of response string\n",
      "    '''\n",
      "    \n",
      "    return pd.DataFrame([{'id': article.get('_id'),\n",
      "                          'source': article.get('source'),\n",
      "                          'news_desk': article.get('news_desk'),\n",
      "                          'blog': article.get('blog'),\n",
      "                          'pub_date': article.get('pub_date'),\n",
      "                          'url': article.get('web_url'),\n",
      "                          'headline': article.get('headline').get('main'),\n",
      "                          'seo_headline': article.get('headline').get('seo'),\n",
      "                          'author': article.get('byline').get('original')[3:],\n",
      "                          'abstract': article.get('abstract'),\n",
      "                          'lead_paragraph': article.get('lead_paragraph'),\n",
      "                          'snippet': article.get('snippet'),\n",
      "                          'word_count': article.get('word_count')}\n",
      "                         for article in data['response']['docs']])\n",
      "    \n",
      "def NYT_article_search(options = {'fq': 'subject:(\"Presidential Election of 2012\") AND type_of_material:(\"News\")',\n",
      "                                  'page': 1,\n",
      "                                  'end_date': '20121106',\n",
      "                                  'api-key': 'b5b3713fb7bf65301ab0efb581a328b4:15:61099097'}):\n",
      "    '''\n",
      "    Retrieves NYT articles from API based on keywords and returns a DataFrame of articles\n",
      "    \n",
      "    Input: dictionary of NYT parameters (http://developer.nytimes.com/docs/read/article_search_api_v2)\n",
      "    Output: Pandas DataFrame of all search results\n",
      "    '''\n",
      "    \n",
      "    # run the query once to determine how many pages to call\n",
      "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
      "    data = json.loads(requests.get(url, params=options).text)\n",
      "    page_limit = data['response']['meta']['hits'] / data['response']['meta']['offset'] + 1\n",
      "    \n",
      "    # parse the first response\n",
      "    NYT_df = parse_NYT_query(data)\n",
      "    \n",
      "    # run the same query page_limit times, parsing into a DataFrame and concatenating the results\n",
      "    for iteration in range(2, page_limit + 1, 1):\n",
      "        \n",
      "        if(iteration % 50 == 0): \n",
      "            print \"Pages complete: %i / %i\" % iteration, page_limit\n",
      "        \n",
      "        # append current page to options dictionary\n",
      "        options['page'] = iteration\n",
      "        data = requests.get(url, params=options).text\n",
      "        data = json.loads(data)\n",
      "        \n",
      "        # parse data into a DataFrame and concat with original DataFrame\n",
      "        NYT_df = NYT_df.append(parse_NYT_query(data), ignore_index=True)\n",
      "    \n",
      "    return NYT_df\n",
      "\n",
      "test = NYT_article_search()\n",
      "\n",
      "# TODO -- AP News, US Today, etc. etc."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(20, 13)\n",
        "(30, 13)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# various functions from the homeworks\n",
      "\n",
      "def get_poll_xml(poll_id):\n",
      "    '''\n",
      "    Downloads xml data from RCP and returns it as a string\n",
      "    '''\n",
      "    \n",
      "    # try to convert poll id to an int\n",
      "    try:\n",
      "        poll_id = int(poll_id)\n",
      "    except:\n",
      "        raise Exception(\"poll_id must be convertable to an int\")        \n",
      "    \n",
      "    # download xml data\n",
      "    return requests.get(\"http://charts.realclearpolitics.com/charts/\"+str(poll_id)+\".xml\").text\n",
      "\n",
      "def rcp_poll_data(xml):\n",
      "    '''\n",
      "    Takes RCP poll xml data as a string and parses it into a pandas dataframe\n",
      "    '''\n",
      "    \n",
      "    # declare dom object to begin parsing the data\n",
      "    dom = web.Element(xml)\n",
      "    \n",
      "    # parse data from <series> (date info) into a list\n",
      "    date_list = [value.content for value in dom('series value')]\n",
      "    \n",
      "    # incorporate date list into pd.Dataframe\n",
      "    poll_dataframe = pd.DataFrame({'date': pd.to_datetime(date_list)})\n",
      "    \n",
      "    # place each graph into a separate entry within the dataframe, {title: list}\n",
      "    for graph in dom.by_tag('graph'):\n",
      "\n",
      "        # generate a list with data from values\n",
      "        data = []\n",
      "        for value in graph('value'):\n",
      "            # if value can be converted to float, add it to dict. Otherwise, add NaN.\n",
      "            try:\n",
      "                data.append(float(value.content))\n",
      "            except:\n",
      "                data.append(float('NaN'))\n",
      "        \n",
      "        # enter the generated list into the dict -- {title : data}\n",
      "        poll_dataframe[graph.attributes['title']] = data\n",
      "    \n",
      "    return poll_dataframe\n",
      "\n",
      "# governor race scraping (if needed)\n",
      "\n",
      "def find_governor_races(html):\n",
      "    '''\n",
      "    Find and return links to RCP races\n",
      "    '''\n",
      "            \n",
      "    # declare dom object to begin parsing the data\n",
      "    dom = web.Element(html) \n",
      "    \n",
      "    # define match types for fnmatch in url patterns\n",
      "    digit = '[0123456789]'\n",
      "    letter = '[abcdefghijklmnopqrstuvwxyz]'\n",
      "    url_pattern = 'http://www.realclearpolitics.com/epolls/'+4*digit+'/governor/'+2*letter+'/*-'+4*digit+'.html'\n",
      "    \n",
      "    # iterate through all <option> urls, adding it to the list only if the url matches the pattern\n",
      "    option_url_list = [url.attributes['value'] for url in dom('option') if (fnmatch(url.attributes['value'], url_pattern))]\n",
      "    \n",
      "    # iterate through all <a> urls, adding them if href attribute exists and the url matches the pattern\n",
      "    href_url_list = [url.attributes.get('href') for url in dom('a') if (fnmatch(str(url.attributes.get('href')), url_pattern))]\n",
      "    \n",
      "    # return the urls, removing duplicates\n",
      "    return list(set(option_url_list + href_url_list))\n",
      "\n",
      "def race_result(url):\n",
      "    '''\n",
      "    Parses and returns the normalized percentage won by each candidate\n",
      "    '''\n",
      "    \n",
      "    # download data from url\n",
      "    html = requests.get(url).text\n",
      "    \n",
      "    # declare dom object to begin parsing the data\n",
      "    dom = web.Element(html) \n",
      "    \n",
      "    # pull header row and first data row from data table\n",
      "    header_row = dom('table.data tr th')\n",
      "    data_row = dom('table.data tr.final td')\n",
      "    \n",
      "    # the number of candidates is four less than the number of columns in the data table\n",
      "    number_candidates = len(header_row) - 4\n",
      "\n",
      "    # take the 4th - end candidate name and remove the party affiliation (splitting on \" (\" to allow for 2-word names\n",
      "    candidates = [str(header_row[3+i].content).split(\" (\")[0] for i in range(number_candidates)]\n",
      "    \n",
      "    # generate result list, taking the 4th - end result, casting it into a float\n",
      "    results = [float(data_row[3+i].content) for i in range(number_candidates)]\n",
      "\n",
      "    # generate a dict of results, normalizing each value by dividing it by the total poll result\n",
      "    results_dict = {}\n",
      "    for i in range(number_candidates):\n",
      "        results_dict[candidates[i]] = 100*results[i]/sum(results)\n",
      "        \n",
      "    return results_dict\n",
      "\n",
      "# All of the Gallup and poll aggregation stuff -- we will probably have to write our own\n",
      "\n",
      "# Networks and Congress stuff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Sentiment Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sentiment algorithm comparison etc.\n",
      "\n",
      "# scrape/ use APIs to find all news articles and rate them on whether they\n",
      "    #  relate to a particular candidate and on their favorability\n",
      "\n",
      "# is the any way we can include TV coverage? Headlines or something?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Media Favorability Scores"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate a weighted media favorability score at any given time\n",
      "    # more recent stories have higher weight, higher impact (how to measure?) news have higher weight\n",
      "\n",
      "# plot media favorability scores over time -- what are the trends? Lead / lag changes in polling?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Election Predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# predict election results based on favorability score\n",
      "\n",
      "# predict election results on polling\n",
      "\n",
      "# predict on both\n",
      "\n",
      "# compare results -- which are the most accurate X weeks out from the election?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Media Movers and Shakers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# network analysis to identify most important news outlets\n",
      "\n",
      "# which individual news outlet is the most accurate at predicting results?\n",
      "    # answer: the Onion"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "*css tweaks in this cell*\n",
      "<style>\n",
      "div.text_cell_render {\n",
      "    line-height: 150%;\n",
      "    font-size: 110%;\n",
      "    width: 800px;\n",
      "    margin-left:50px;\n",
      "    margin-right:auto;\n",
      "    }\n",
      "</style>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}