{"name":"Media Bias in the 2012 Presidential Election: Did the Media Really Help Obama Win?","tagline":"CS109 Final Project - Jonathan Marks, Daniel Taylor, Michael Suo, Ben Zauzmer","body":"#Project Summary\r\n###Background\r\nVarious interesting things about news and elections go here...\r\n\r\n###Methods\r\nVarious interesting things about our methods go here...\r\n\r\n###Results\r\nVarious results...\r\n\r\n###Conculusions\r\nThings we discovered are the most important things ever discovered...\r\n\r\n#Data Collection\r\n##Web Scraping and API Parsing\r\nOur project attempts to analyze the effect of national media (particularly daily newspapers) on the 2012 Presidential Election. To begin, we first developed web scrapers / API parsers for ten of the top fifteen daily newspapers by daily circulation that filtered articles on date and relevance to the 2012 election. While most newspapers do not allow for unrestricted access to full articles, all of our data sources return article headlines, authors, word count, section, and a short abstract or summary of the article. We will use this data to perform natural language processing (NLP) of the article dataset and evaluate the impact of electoral news coverage.\r\n\r\n\r\n###Primary Data Sources:\r\n* New York Times API\r\n* The Guardian API\r\n* USA Today API + archive scraping\r\n* The Wall Street Journal archive scraping\r\n* New York Daily News scraping\r\n* The Boston Globe archive scraping\r\n* The Los Angeles Times archive scraping\r\n* The Washington Post archive scraping\r\n* The Chicago Tribune archive scraping\r\n* Newsday archive scraping\r\n* Archive Scraping of several Pennsylvania Newspapers (Philadelphia Herald, Daily News, etc.)\r\n\r\nWe also wrote an interface to Bing News but after running out of free queries Microsoft was unable to grant us additional free queries.\r\n\r\n##Data Munging\r\nAfter combining all the datasets of our scraped data, we filtered it to ensure that all articles are related to Romney and Obama based on string matching. We then split the corpus of articles into two groups: one directly related to Romney and the other directly related to Obama. To do this, we used [Daniel's Algorithm].\r\n\r\nFollowing filtering and joining the dataset, we have a total of 35,274 articles. The relative article counts among different newspapers are as follows:\r\n\r\n... descriptive data of entire dataset ...\r\n\r\n#Sentiment analysis\r\nThe data that we have collected so far does not have an intrinsic categorization built-in like the Rotten Tomatoes data from HW3. Instead, we will use three different precategorized datasets to evaluate each article on positivity, subjectivity, and support.\r\n\r\nThe positivity dataset is a set of 50,000 IMDB movie reviews from Stanford (url) that are presplit into testing and training data. While our dataset is composed of news articles, not movies, we will be using ngrams of at most length 3 and the general trend of whether a phrase connotes a positive feeling about a topic or not should be approximately the same.\r\n\r\nThe subjectivity dataset is composed of a set of 7419 IMDB movie summaries and Rotten Tomatoes reviews -- an IMDB summary is considered objective while Rotten Tomatoes data is considered subjective.\r\n\r\nThe support/oppose dataset comes from 10,000 political speeches that were precharacterized into support/oppose.\r\n\r\n... distributions etc. ...\r\n\r\n##Vectorization\r\nHaving loaded in the training data, we now vectorize the data and convert it into a form that we can use for classification. There are two different vectorizers that we can use: CountVectorizer, which simply converts the string 'Romney is running for election' into a bag of words with a count of each word in the string. Tf-idf term weighting, however, re-weights the counts based on the frequency of words: e.g. words like 'the' or 'is' will receive a much lower weighting in the final analysis because these terms occur too frequently to be informative.\r\n\r\nWe will use sparse vectors throughout this analysis to save on memory space.\r\n\r\n##Naive Bayes Training\r\nWe now use the vectorized data to fit a multinomial Naive Bayes classfier. First, however, we split the three datasets into testing and training data. We then fit the classifier and report the classification scores to determine the degree of overfitting.\r\n\r\nIn our preliminary analysis, we will use a CountVectorizer with unigrams. We will then look at more advanced options later.\r\n\r\n... comparison of different models, etc.\r\n\r\n##Cross Validation\r\nTo improve the fit of this classifier, we can use cross-validation to determine the best constants for fitting.\r\n\r\nFirst, we will attempt to cross-validate by maximizing the testing set score. We will then attempt to maximize the log-liklihood and we will compare the results.\r\n\r\nIn cross validation, for each dataset we will compare Tdif vs. Counts, unigrams vs. bigrams, alpha values (0-50), and min_df values (1e-5 through 1e-1).\r\n\r\n... etc. etc. etc. ...","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}