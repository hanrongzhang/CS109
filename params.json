{"name":"Media Bias in the 2012 Presidential Election: Did the Media Really Help Obama Win?","tagline":"CS109 Final Project - Jonathan Marks, Daniel Taylor, Michael Suo, Ben Zauzmer","body":"<img src=\"https://raw.github.com/jmarks1992/CS109/master/newspapers.png\">\r\n\r\n###Background\r\nAfter President Barack Obama was reelected on November 6, 2012, it took Fox News just one day to publish an article titled <a href=\"http://www.foxnews.com/opinion/2012/11/07/five-ways-mainstream-media-tipped-scales-in-favor-obama/\">\"Five ways the mainstream media tipped the scales in favor of Obama.\"</a> The left was just as blunt, such as Salon's article two months later, <a href=\"http://www.salon.com/2013/01/05/12_most_despicable_things_fox_news_did_in_2012/\">\"12 most despicable things Fox News did in 2012.\"</a>\r\n\r\nBoth sides in this debate are correct that a functioning democracy relies heavily on a fair and free press during elections, so it is important to use the data from 2012 to answer a few key questions: How does Presidential election coverage among major media outlets? Which ones print articles that are the most positive for one candidate or the other, or the most supportive, or the most subjective? Is there a distincitive style of writing associated with an individual newspaper?\r\n\r\n\r\n###Methods\r\nWe gathered 2011-2012 article data from many major news sources. Specifically, we used APIs from the New York Times, USA Today, The Guardian, and Bing. We wrote web scrapers to gather data from the Wall Street Journal, the New York Daily News, Boston Globe, the Washington Post, and the Los Angeles Times. We then cleaned the data so that all of the articles from different sources could be standardized and combined into one dataframe containing over 30,000 rows.\r\n\r\nWe wrote a script to determine which, if either, of the two candidates (Obama or Romney) the article related to. We also used both pre-written sentiment databases and manual sentiment training to determine the probability that each article is positive for the candidate it relates to, that it supports that candidate, and how subjective or objective it is.\r\n\r\nNext, we cross-validated our data to choose the optimum of 240 combinations, looping through different scoring methods, vectorizers, n-grams, min_dfs, and alphas. After intial cross validation, we redefined the parameter search space to more accurately encompass the peak scores and reperformed cross validation.\r\n\r\nFinally, we visualized our data set in a variety of ways to determine what insights we could find about 2012 election coverage. We graphed article counts by source, candidate, and date. We graphed the frequencies of word count, abstract length, and headline length. We graphed our sentiment analysis results by source, candidate, and date. We also used contour plots and clustering to examine how closely related articles from the same source are to each other.\r\n\r\n\r\n###Results\r\nContrary to the hyperbolic claims of media bias following the 2012 election, we found a surprising lack of difference in the positivity and support of candidates across different newspapers and across time. While some metrics, most notably media subjectivity, did increase as the election came closer, there does not appear to be a consistent bias for or against either candidate within the news media as a whole. Similarly, there is not a significant relationship between polling results for each candidate and the media's portrayal of that candidate. This suggests the media did not, at least in a way noticable by our tests, tip the scales in favor of Barack Obama.\r\n\r\n\r\n###Conculusions\r\nThere are two ways to explain these findings. The more cynical one is that newspapers want to make coverage more exciting as the election approaches to sell more papers, so they become more subjective over time. A more realistic justification is that there is simply more election coverage as the big day approaches - the objectives coverage is still the same, so most of the additional coverage is filled with subjective pieces.\r\n\r\nIn summary, the outpouring of media coverage concerning media bias and the effect that differential coverage had on the election's results seems to be overblown. While one can easily point at particularly devisive or slanted pieces, looking at newspaper coverage as a whole there does not seem to be a large difference between the media's coverage of Barack Obama or Mitt Romney.\r\n\r\nTo see our complete project file, please [go here](http://nbviewer.ipython.org/github/jmarks1992/CS109/blob/master/Project.ipynb?create=1). A summary of our methods and findings is below.\r\n\r\n---\r\n\r\n#Data Collection\r\n\r\n\r\n##Web Scraping and API Parsing\r\nOur project attempts to analyze the effect of national media (particularly daily newspapers) on the 2012 Presidential Election. To begin, we first developed web scrapers / API parsers for ten of the top fifteen daily newspapers by daily circulation that filtered articles on date and relevance to the 2012 election. While most newspapers do not allow for unrestricted access to full articles, all of our data sources return article headlines, authors, word count, section, and a short abstract or summary of the article. We will use this data to perform natural language processing (NLP) of the article dataset and evaluate the impact of electoral news coverage.\r\n\r\n\r\n###Primary Data Sources:\r\n* New York Times API\r\n* The Guardian API\r\n* USA Today API + archive scraping\r\n* The Wall Street Journal archive scraping\r\n* New York Daily News scraping\r\n* The Boston Globe archive scraping\r\n* The Los Angeles Times archive scraping\r\n* The Washington Post archive scraping\r\n* The Chicago Tribune archive scraping\r\n* Newsday archive scraping\r\n* Archive Scraping of several Pennsylvania Newspapers (Philadelphia Herald, Daily News, etc.)\r\n\r\nWe also wrote an interface to Bing News but after running out of free queries Microsoft was unable to grant us additional free queries.\r\n\r\n\r\n##Data Munging\r\nAfter combining all the datasets of our scraped data, we filtered it to ensure that all articles are related to Romney and Obama based on string matching. We then split the corpus of articles into two groups: one directly related to Romney and the other directly related to Obama. To do this, we used [Daniel's Algorithm].\r\n\r\nFollowing filtering and joining the dataset, we have a total of 35,274 articles.\r\n\r\nWe then visualized our dataset in a few different ways:\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/1.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/2.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/5.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/6.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/23.png\">\r\n\r\n---\r\n\r\n#Sentiment analysis\r\nThe data that we have collected so far does not have an intrinsic categorization built-in like the Rotten Tomatoes data from HW3. Instead, we will use three different precategorized datasets to evaluate each article on positivity, subjectivity, and support.\r\n\r\nThe positivity dataset is a set of 50,000 IMDB movie reviews from Stanford (http://ai.stanford.edu/~amaas/data/sentiment/) that are presplit into testing and training data. While our dataset is composed of news articles, not movies, we will be using ngrams of at most length 3 and the general trend of whether a phrase connotes a positive feeling about a topic or not should be approximately the same.\r\n\r\nThe subjectivity dataset is composed of a set of 7419 IMDB movie summaries and Rotten Tomatoes reviews -- an IMDB summary is considered objective while Rotten Tomatoes data is considered subjective (http://www.cs.cornell.edu/people/pabo/movie-review-data).\r\n\r\nThe support/oppose dataset comes from 10,000 political speeches that were precharacterized into support/oppose. This should provide us with a \"politically slanted\" sentiment analysis that can expose or check back against the possibility that movie reviews are qualititively different in structure than political articles (http://www.cs.cornell.edu/home/llee/data/convote.html).\r\n\r\n\r\n##Vectorization\r\nHaving loaded in the training data, we then vectorized the data and convert it into a form that we can use for classification. There are two different vectorizers that we can use: CountVectorizer, which simply converts the string 'Romney is running for election' into a bag of words with a count of each word in the string. Tf-idf term weighting, however, re-weights the counts based on the frequency of words: e.g. words like 'the' or 'is' will receive a much lower weighting in the final analysis because these terms occur too frequently to be informative.\r\n\r\n\r\n##Naive Bayes Training\r\nWe then used the vectorized data to fit a multinomial Naive Bayes classfier. First, however, we split the three datasets into testing and training data. We then fit the classifier and reported the classification scores to determine the degree of overfitting.\r\n\r\nIn our preliminary analysis, we used a CountVectorizer with unigrams (single words only), then proceeded to Tf-idf vectorization with bigrams (single words + sets of two words). To score each of the different options for fit, we used both a quantitative tool (fit accuracy scores) and a qualitative tool (a fit visualization -- best fits should have a 1:1 relationship between P(positive) and actual % positive).\r\n\r\nOne example of a poorly fit dataset is below:\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/14.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/15.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/16.png\">\r\n\r\nAll three of these examples are poorly fit: we do not see a 1:1 relationship between the model and reality and the distributions of counts at a particular probability are basically unimodal.\r\n\r\n\r\n##Cross Validation\r\nTo improve the fit of this classifier, we can use cross-validation to determine the best constants for fitting. Cross validation in this context means searching a parameter space to maximize a score that relates goodness of fit of a model. We performed cross-validation twice: once to obtain a rough idea of the maximum location, and a second time to find that peak more exactly. We compared Tdif vs. Counts, unigrams vs. bigrams, alpha values (0-50), and min_df values (1e-5 through 1e-1). To visualize this search, we made a visualization of a contour plot of fit scores based on alpha, min_df, Tdif/Counts, and unigrams/bigrams. We also report below the best fit parameters for the data.\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/17.png\">\r\n\r\n\r\n###Positivity Post Cross Validation\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>alpha</th>\r\n      <th>avg_score</th>\r\n      <th>min_df</th>\r\n      <th>ngram</th>\r\n      <th>scores</th>\r\n      <th>scoring_method</th>\r\n      <th>vectorizer</th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>212</th>\r\n      <td> 0.05</td>\r\n      <td>    0.895300</td>\r\n      <td> 0.000001</td>\r\n      <td> (1, 2)</td>\r\n      <td> [0.895602087958, 0.897042059159, 0.893255730229]</td>\r\n      <td>  testing_score</td>\r\n      <td> TfidfVectorizer</td>\r\n    </tr>\r\n    <tr>\r\n      <th>110</th>\r\n      <td> 0.05</td>\r\n      <td>-4288.055188</td>\r\n      <td> 0.000050</td>\r\n      <td> (1, 2)</td>\r\n      <td> [-4330.25132721, -4234.31419617, -4299.60003994]</td>\r\n      <td> log_likelihood</td>\r\n      <td> TfidfVectorizer</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\n\r\n###Support / Oppose Post Cross Validation\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>alpha</th>\r\n      <th>avg_score</th>\r\n      <th>min_df</th>\r\n      <th>ngram</th>\r\n      <th>scores</th>\r\n      <th>scoring_method</th>\r\n      <th>vectorizer</th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>236</th>\r\n      <td> 0.05</td>\r\n      <td>    0.753201</td>\r\n      <td> 0.0001</td>\r\n      <td> (1, 2)</td>\r\n      <td>   [0.76425394258, 0.74322684998, 0.752122927618]</td>\r\n      <td>  testing_score</td>\r\n      <td> TfidfVectorizer</td>\r\n    </tr>\r\n    <tr>\r\n      <th>117</th>\r\n      <td> 0.10</td>\r\n      <td>-1291.768700</td>\r\n      <td> 0.0001</td>\r\n      <td> (1, 2)</td>\r\n      <td> [-1278.80734317, -1298.52353558, -1297.97522042]</td>\r\n      <td> log_likelihood</td>\r\n      <td> TfidfVectorizer</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\n\r\n###Subjectivity Post Cross Validation\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>alpha</th>\r\n      <th>avg_score</th>\r\n      <th>min_df</th>\r\n      <th>ngram</th>\r\n      <th>scores</th>\r\n      <th>scoring_method</th>\r\n      <th>vectorizer</th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>225</th>\r\n      <td> 0.10</td>\r\n      <td>   0.924800</td>\r\n      <td> 0.00001</td>\r\n      <td> (1, 2)</td>\r\n      <td> [0.925314937013, 0.927692769277, 0.921392139214]</td>\r\n      <td>  testing_score</td>\r\n      <td> TfidfVectorizer</td>\r\n    </tr>\r\n    <tr>\r\n      <th>104</th>\r\n      <td> 0.05</td>\r\n      <td>-707.571337</td>\r\n      <td> 0.00001</td>\r\n      <td> (1, 2)</td>\r\n      <td> [-689.981885375, -695.881432056, -736.850693017]</td>\r\n      <td> log_likelihood</td>\r\n      <td> TfidfVectorizer</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\n\r\n###Results Post Cross Validation\r\nFollowing cross validation, these classifiers have greatly improved: there is much less consistent bias and the models are typically neither under or overconfident. A fit plot of the three datasets with optimized parameters is below:\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/18.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/19.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/20.png\">\r\n\r\n\r\n##Applying Training Data to Entire Dataset\r\nHaving found a model that predicts variance well in the training data, we now applied that model to the actual corpus of articles. The results were as follows:\r\n\r\n\r\nIn table format:\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>probability_positive</th>\r\n      <th>probability_support</th>\r\n      <th>probability_subjective</th>\r\n      <th>word_count</th>\r\n    </tr>\r\n    <tr>\r\n      <th>candidate</th>\r\n      <th></th>\r\n      <th></th>\r\n      <th></th>\r\n      <th></th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>Obama</th>\r\n      <td> 0.606889</td>\r\n      <td> 0.401296</td>\r\n      <td> 0.174573</td>\r\n      <td> 742.270738</td>\r\n    </tr>\r\n    <tr>\r\n      <th>Romney</th>\r\n      <td> 0.638615</td>\r\n      <td> 0.403030</td>\r\n      <td> 0.191612</td>\r\n      <td> 864.380741</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\nGraphically, we see very little difference between the media positivity in its coverage of Romney and Obama: the average scores are practically the same across the board and the distribution of scores is very similar.\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/21.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/22.png\">\r\n\r\n\r\n---\r\n\r\n#Untrained Clustering\r\nAnother way of determining how related articles are is through clustering. In this example, we clustered by source. We then printed five different metrics to determine how clustered the abstracts are, where the abstracts are transformed by a vectorizer into arrays. We also used PCA with two components in order to display the data as an image, which gave us a reasonably estimated visualization for how closely related the data is.\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/24.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/25.png\">\r\n\r\n<img src=\"https://raw.github.com/jmarks1992/CS109/master/Images/26.png\">\r\n\r\n\r\nIn the first of the two clustering demonstrations above, we used a count vectorizer, which assigns a 0 or 1 to each word based on whether or not it appears in the abstract. In the second, we used IDF, which penalizes words based on how often they appear. The idea is that words like \"a\" or \"the\" should probably not be as influential as other words in determining appropriate cluster, but probably appear very often.\r\n\r\nNote that in each example, the homogeneity (on a scale of 0 to 1) is quite low, meaning each cluster contains articles form many other sources. The completeness (also on a scale of 0 to 1), on the other hand, is higher. This means we are closer to achieving the goal that all articles from the same source are located within the same cluster. The fact that their V-measure - the harmonic mean of the two variables - is so low indicates that overall clustering on either vectorizer fails to adequately separate the data by source.\r\n\r\nBased on experimentation with clustering on other subsets of the data, over different time frames, and on other variables besides the source, the general negative result is the same. This may be due to the fact that abstracts are relatively small amounts of text, so there may not be enough clues in any one abstract to distinguish which category it belongs in.\r\n\r\n---\r\n\r\n#Media Favorability Scores and Polling\r\nFROM DANIEL","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}