{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#CS109 Final Project\n",
      "###Ben Zauzmer, Daniel Taylor, Jonathan Marks, and Michael Suo\n",
      "\n",
      "---\n",
      "\n",
      "###Background\n",
      "Various interesting things about news and elections go here...\n",
      "\n",
      "###Methods\n",
      "Various interesting things about our methods go here...\n",
      "\n",
      "###Results\n",
      "Various results...\n",
      "\n",
      "###Conculusions\n",
      "Things we discovered are the most important things ever discovered..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data Collection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from collections import defaultdict\n",
      "import json\n",
      "import requests\n",
      "import warnings\n",
      "from pattern import web\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import pprint\n",
      "import datetime\n",
      "from datetime import date, timedelta as td\n",
      "\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib as mpl\n",
      "\n",
      "import random\n",
      "import math\n",
      "from scipy import stats\n",
      "import brewer2mpl\n",
      "\n",
      "#colorbrewer2 Dark2 qualitative color table\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'white'\n",
      "rcParams['patch.facecolor'] = dark2_colors[0]\n",
      "rcParams['font.family'] = 'StixGeneral'\n",
      "\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()\n",
      "        \n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: No traceback has been produced, nothing to debug.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Scraping / API access to do list:\n",
      "* New York Times API search (done, Jonathan)\n",
      "* Wall Street Journal archive scraping (done, Jonathan)\n",
      "* USA Today API search (in progress, Jonathan)\n",
      "* The Guardian API (http://www.theguardian.com/open-platform)\n",
      "* The Washington Post (deprecated API, scraping?)\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###New York Times API"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_NYT_query(data):\n",
      "    '''\n",
      "    Takes an inputted JSON-loaded string of NYT API results and parses it into a DataFrame\n",
      "    \n",
      "    Input: JSON-loaded NYT response string\n",
      "    Output: Pandas DataFrame of response string\n",
      "    '''\n",
      "    \n",
      "    results = []\n",
      "    for article in data.get('response').get('docs'):\n",
      "        result = {'id': article.get('_id'),\n",
      "                  'source': article.get('source'),\n",
      "                  'news_desk': article.get('news_desk'),\n",
      "                  'blog': article.get('blog'),\n",
      "                  'pub_date': article.get('pub_date'),\n",
      "                  'url': article.get('web_url'),\n",
      "                  'abstract': article.get('abstract'),\n",
      "                  'lead_paragraph': article.get('lead_paragraph'),\n",
      "                  'snippet': article.get('snippet'),\n",
      "                  'word_count': article.get('word_count'),\n",
      "                  'keywords': article.get('keywords')}\n",
      "        \n",
      "        # try parsing headline, otherwise simply pass entire element\n",
      "        try:\n",
      "            result['headline'] = article.get('headline').get('main')\n",
      "            result['seo_headline'] = article.get('headline').get('seo')\n",
      "        except:\n",
      "            result['headline'] = article.get('headline')\n",
      "            result['seo_headline'] = article.get('headline')\n",
      "        \n",
      "        # try parsing author, otherwise simply pass entire element\n",
      "        try:\n",
      "            result['author'] = article.get('byline').get('original')[3:]\n",
      "        except:\n",
      "            result['author'] = article.get('byline')\n",
      "        \n",
      "        # append the results to growing list\n",
      "        results.append(result)\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "    \n",
      "def NYT_article_search(options = {'fq': 'subject:(\"Presidential Election of 2012\") AND type_of_material:(\"News\")',\n",
      "                                  'end_date': '20121106'},\n",
      "                       api_key = 'b5b3713fb7bf65301ab0efb581a328b4:15:61099097',\n",
      "                       full_query = False,\n",
      "                       start_page = 1,\n",
      "                       print_result = False):\n",
      "    '''\n",
      "    Retrieves NYT articles from API based on keywords and returns a DataFrame of articles\n",
      "    \n",
      "    Input: dictionary of NYT parameters (http://developer.nytimes.com/docs/read/article_search_api_v2)\n",
      "        q: search word\n",
      "        fq: field query. Example: 'fq': 'subject:(\"Presidential Election of 2012\") AND type_of_material:(\"News\")'\n",
      "        begin_date: YYYYMMDD\n",
      "        end_date: YYYYMMDD\n",
      "        ...etc.\n",
      "    Output: Pandas DataFrame of all search results\n",
      "    '''\n",
      "    \n",
      "    # run the query once to determine how many pages to call\n",
      "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
      "    options['page'] = start_page\n",
      "    options['api-key'] = api_key\n",
      "    data = json.loads(requests.get(url, params=options).text)\n",
      "    \n",
      "    # print the data if asked for it\n",
      "    if(print_result): pprint.pprint(data)\n",
      "    \n",
      "    # calculate the page limit -- max allowed currently by NYT is 100\n",
      "    page_limit = data['response']['meta']['hits'] / data['response']['meta']['offset'] + 1\n",
      "    if (page_limit > 100):\n",
      "        print \"NYT allows for up to 100 pages to be queried; request of %i will be truncated to 100.\" % page_limit\n",
      "        page_limit = 100\n",
      "    \n",
      "    # parse the first response\n",
      "    NYT_df = parse_NYT_query(data)\n",
      "    \n",
      "    # if full query desired, run the same query page_limit times, parsing into a DataFrame and concatenating the results\n",
      "    if(full_query):\n",
      "        for iteration in range(2, page_limit + 1, 1):\n",
      "            \n",
      "            if(iteration % 25 == 0): \n",
      "                print \"Pages complete: %i / %i\" % (iteration, page_limit)\n",
      "            \n",
      "            # append current page to options dictionary\n",
      "            options['page'] = iteration\n",
      "            options['api-key'] = api_key\n",
      "            data = requests.get(url, params=options).text\n",
      "            data = json.loads(data)\n",
      "            \n",
      "            # attempt to parse response, if successful, add to DataFrame. If not, print it.\n",
      "            try:\n",
      "                NYT_df = NYT_df.append(parse_NYT_query(data), ignore_index=True)\n",
      "            except:\n",
      "                pprint.pprint(data)\n",
      "    \n",
      "    return NYT_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NYT -- actual query removed so that we don't use up API space unnecessarily\n",
      "'''\n",
      "election_2012_NYT = NYT_article_search(full_query = True,\n",
      "                                       options = {'fq': 'subject:(\"Presidential Election of 2012\") AND type_of_material:(\"News\")',\n",
      "                                       'end_date': '20121106'})\n",
      "\n",
      "election_2012_NYT.to_csv(\"./NYT_data/election_2012_NYT.csv\", encoding = \"UTF-8\")\n",
      "'''\n",
      "\n",
      "election_2012_NYT = pd.read_csv(\"./NYT_data/election_2012_NYT.csv\", encoding = \"UTF-8\")\n",
      "election_2012_NYT = election_2012_NYT.drop('Unnamed: 0', 1)\n",
      "election_2012_NYT.head(4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>abstract</th>\n",
        "      <th>author</th>\n",
        "      <th>blog</th>\n",
        "      <th>headline</th>\n",
        "      <th>id</th>\n",
        "      <th>keywords</th>\n",
        "      <th>lead_paragraph</th>\n",
        "      <th>news_desk</th>\n",
        "      <th>pub_date</th>\n",
        "      <th>seo_headline</th>\n",
        "      <th>snippet</th>\n",
        "      <th>source</th>\n",
        "      <th>url</th>\n",
        "      <th>word_count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Former Pres Bill Clinton continues his campaig...</td>\n",
        "      <td>                    MARK LEIBOVICH</td>\n",
        "      <td> []</td>\n",
        "      <td> Voice Is Strained, but Support on the Trail Un...</td>\n",
        "      <td> 50971f0995e998af7118b44d</td>\n",
        "      <td> [{u'value': u'Obama, Barack', u'is_major': u'N...</td>\n",
        "      <td> Whoever wins, the campaign has solidified (or ...</td>\n",
        "      <td>        National</td>\n",
        "      <td> 2012-11-05T00:00:00Z</td>\n",
        "      <td> Bill Clinton Presses On in Campaign for Barack...</td>\n",
        "      <td> Whoever wins, the campaign has solidified (or ...</td>\n",
        "      <td> The New York Times</td>\n",
        "      <td> http://www.nytimes.com/2012/11/05/us/politics/...</td>\n",
        "      <td> 1186</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> \"Pres Obama and Mitt Romney hunt for last-minu...</td>\n",
        "      <td> MARK LANDLER and MICHAEL D. SHEAR</td>\n",
        "      <td> []</td>\n",
        "      <td> As Candidates Make Final Pleas, Legal Battles ...</td>\n",
        "      <td> 5096ad5095e998af7118b35a</td>\n",
        "      <td> [{u'value': u'Elections', u'is_major': u'Y', u...</td>\n",
        "      <td> President Obama and Mitt Romney hunted for sup...</td>\n",
        "      <td>        National</td>\n",
        "      <td> 2012-11-05T00:00:00Z</td>\n",
        "      <td> Obama and Romney Campaigns Begin Skirmish Over...</td>\n",
        "      <td> President Obama and Mitt Romney hunted for sup...</td>\n",
        "      <td> The New York Times</td>\n",
        "      <td> http://www.nytimes.com/2012/11/05/us/politics/...</td>\n",
        "      <td> 1289</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> ASHLEY PARKER and MICHAEL BARBARO</td>\n",
        "      <td> []</td>\n",
        "      <td>            Romney, a Challenger at the Crossroads</td>\n",
        "      <td> 5097450d95e998af7118b49e</td>\n",
        "      <td> [{u'rank': u'3', u'name': u'subject', u'value'...</td>\n",
        "      <td> Mitt Romney has arrived at a strange and unfam...</td>\n",
        "      <td> U.S. / Politics</td>\n",
        "      <td> 2012-11-05T00:00:00Z</td>\n",
        "      <td>            Romney, a Challenger at the Crossroads</td>\n",
        "      <td> Mitt Romney has arrived at a strange and unfam...</td>\n",
        "      <td> The New York Times</td>\n",
        "      <td> http://www.nytimes.com/2012/11/05/us/politics/...</td>\n",
        "      <td> 1534</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td>                       PETER BAKER</td>\n",
        "      <td> []</td>\n",
        "      <td>              A President\u2019s Last Race, Win or Lose</td>\n",
        "      <td> 5097450d95e998af7118b49f</td>\n",
        "      <td> [{u'rank': u'5', u'name': u'persons', u'value'...</td>\n",
        "      <td> President Obama\u2019s checklist of \u201clasts\u201d grew lo...</td>\n",
        "      <td> U.S. / Politics</td>\n",
        "      <td> 2012-11-05T00:00:00Z</td>\n",
        "      <td> Obama Takes Frenetic, Final Campaign Days in S...</td>\n",
        "      <td> President Obama\u2019s checklist of \u201clasts\u201d grew lo...</td>\n",
        "      <td> The New York Times</td>\n",
        "      <td> http://www.nytimes.com/2012/11/05/us/politics/...</td>\n",
        "      <td> 1584</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "                                            abstract                             author blog                                           headline                        id                                           keywords                                     lead_paragraph        news_desk              pub_date                                       seo_headline                                            snippet              source                                                url  word_count\n",
        "0  Former Pres Bill Clinton continues his campaig...                     MARK LEIBOVICH   []  Voice Is Strained, but Support on the Trail Un...  50971f0995e998af7118b44d  [{u'value': u'Obama, Barack', u'is_major': u'N...  Whoever wins, the campaign has solidified (or ...         National  2012-11-05T00:00:00Z  Bill Clinton Presses On in Campaign for Barack...  Whoever wins, the campaign has solidified (or ...  The New York Times  http://www.nytimes.com/2012/11/05/us/politics/...        1186\n",
        "1  \"Pres Obama and Mitt Romney hunt for last-minu...  MARK LANDLER and MICHAEL D. SHEAR   []  As Candidates Make Final Pleas, Legal Battles ...  5096ad5095e998af7118b35a  [{u'value': u'Elections', u'is_major': u'Y', u...  President Obama and Mitt Romney hunted for sup...         National  2012-11-05T00:00:00Z  Obama and Romney Campaigns Begin Skirmish Over...  President Obama and Mitt Romney hunted for sup...  The New York Times  http://www.nytimes.com/2012/11/05/us/politics/...        1289\n",
        "2                                                NaN  ASHLEY PARKER and MICHAEL BARBARO   []             Romney, a Challenger at the Crossroads  5097450d95e998af7118b49e  [{u'rank': u'3', u'name': u'subject', u'value'...  Mitt Romney has arrived at a strange and unfam...  U.S. / Politics  2012-11-05T00:00:00Z             Romney, a Challenger at the Crossroads  Mitt Romney has arrived at a strange and unfam...  The New York Times  http://www.nytimes.com/2012/11/05/us/politics/...        1534\n",
        "3                                                NaN                        PETER BAKER   []               A President\u2019s Last Race, Win or Lose  5097450d95e998af7118b49f  [{u'rank': u'5', u'name': u'persons', u'value'...  President Obama\u2019s checklist of \u201clasts\u201d grew lo...  U.S. / Politics  2012-11-05T00:00:00Z  Obama Takes Frenetic, Final Campaign Days in S...  President Obama\u2019s checklist of \u201clasts\u201d grew lo...  The New York Times  http://www.nytimes.com/2012/11/05/us/politics/...        1584"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###USA Today API"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# USA Today (1000 calls a day)\n",
      "def USToday_article_search(options = {'tag': '',\n",
      "                                      'api_key': 'djp3yvuhr5zsun3m9yfybcjx'}):\n",
      "    \n",
      "    # run the query\n",
      "    url = 'http://api.usatoday.com/open/articles'\n",
      "    data = json.loads(requests.get(url, params=options).text)\n",
      "    \n",
      "    # parse data into DataFrame (may require multiple pages as in NYT)\n",
      "    \n",
      "    \n",
      "# TODO -- AP News, US Today, etc. etc."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Wall Street Journal Scraping\n",
      "\n",
      "The WSJ, as far as I can tell, doesn't have an API: however, it does have a nice easily-accessible archive of the form \"http://online.wsj.com/public/page/archive-YYYY-M(M)-D(D).html\", and within each of these pages there is a nice list of articles. I wrote a normal scraping algorithm to grab this data. It's nice because we get all of the articles so we can look at trends such as # times mentioned vs. polling numbers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# WSJ -- Scraping\n",
      "\n",
      "def WSJ_single_page(date):\n",
      "    '''\n",
      "    Scrapes a single page of the WSJ article archive.\n",
      "    \n",
      "    Inputs: YYYY-M(M)-D(D) of desired archive\n",
      "    Output: Pandas DataFrame of results\n",
      "    '''\n",
      "    \n",
      "    # generate URL and load the page\n",
      "    url = 'http://online.wsj.com/public/page/archive-%s.html' % date\n",
      "    html = requests.get(url).text\n",
      "    \n",
      "    # declare dom object to begin parsing the data and initialize DataFrame\n",
      "    dom = web.Element(html)\n",
      "    \n",
      "    WSJ_result_df = pd.DataFrame()\n",
      "    \n",
      "    # iterate through all <li>'s for articles, generating dicts\n",
      "    return pd.DataFrame([{'source': 'Wall Street Journal',\n",
      "                          'headline': article('h2 a')[0].content,\n",
      "                          'url': article('h2 a')[0].attributes.get('href'),\n",
      "                          'snippet': article('p')[0].content,\n",
      "                          'pub_date': pd.datetime(int(date.split('-')[0]), int(date.split('-')[1]), int(date.split('-')[2]))} \n",
      "                          for article in dom('ul.newsItem li')])\n",
      "\n",
      "def WSJ_scrape_archives(start_date, end_date = datetime.datetime.now()):\n",
      "    '''\n",
      "    Scrape all articles, headlines, and descriptions from the Wall Street Journal Archives\n",
      "    \n",
      "    Inputs: begin_date, end_date in YYYY-M(M)-D(D) format (use datetime.datetime(year, month, day))\n",
      "    Output: DataFrame with all inputs appended\n",
      "    '''\n",
      "    \n",
      "    delta = end_date - start_date\n",
      "    \n",
      "    WSJ_full = pd.DataFrame()\n",
      "    \n",
      "    # for every date in the given range, add the new results to a DataFrame\n",
      "    for i in range(delta.days + 1):\n",
      "        \n",
      "        # print update every 30 days (roughly a month)\n",
      "        if (i % 30 == 0):\n",
      "            print \"Currently scraping: \" + (start_date + td(days=i)).strftime(\"%Y-%-m-%-d\")\n",
      "        \n",
      "        # query and parse the page\n",
      "        WSJ_single = WSJ_single_page((start_date + td(days=i)).strftime(\"%Y-%-m-%-d\"))\n",
      "        \n",
      "        # if full is empty, set equal. Otherwise append.\n",
      "        if(WSJ_full.shape[0] > 0):\n",
      "            WSJ_full = WSJ_full.append(WSJ_single, ignore_index = True)\n",
      "        else:\n",
      "            WSJ_full = WSJ_single\n",
      "    \n",
      "    return WSJ_full"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "(29056, 5)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# testing WSJ API\n",
      "'''\n",
      "WSJ_full = WSJ_scrape_archives(datetime.datetime(2009, 4, 1))\n",
      "WSJ_full.to_csv(\"WSJ_full.csv\", encoding = \"UTF-8\")\n",
      "'''\n",
      "WSJ_full = pd.read_csv(\"WSJ_full.csv\", encoding = \"UTF-8\")\n",
      "WSJ_full = WSJ_full.drop('Unnamed: 0', 1)\n",
      "WSJ_full.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Sentiment Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sentiment algorithm comparison etc.\n",
      "\n",
      "# scrape/ use APIs to find all news articles and rate them on whether they\n",
      "    #  relate to a particular candidate and on their favorability\n",
      "\n",
      "# is the any way we can include TV coverage? Headlines or something?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Media Favorability Scores"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate a weighted media favorability score at any given time\n",
      "    # more recent stories have higher weight, higher impact (how to measure?) news have higher weight\n",
      "\n",
      "# plot media favorability scores over time -- what are the trends? Lead / lag changes in polling?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Election Predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# predict election results based on favorability score\n",
      "\n",
      "# predict election results on polling\n",
      "\n",
      "# predict on both\n",
      "\n",
      "# compare results -- which are the most accurate X weeks out from the election?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Media Movers and Shakers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# network analysis to identify most important news outlets\n",
      "\n",
      "# which individual news outlet is the most accurate at predicting results?\n",
      "    # answer: the Onion"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "*css tweaks in this cell*\n",
      "<style>\n",
      "div.text_cell_render {\n",
      "    line-height: 150%;\n",
      "    font-size: 110%;\n",
      "    width: 800px;\n",
      "    margin-left:50px;\n",
      "    margin-right:auto;\n",
      "    }\n",
      "</style>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}